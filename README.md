# Multilayer Perceptron

Este reposit√≥rio cont√©m a implementa√ß√£o completa da Atividade 3 da disciplina de Redes Neurais Artificiais do Programa de P√≥s-Gradua√ß√£o em Ci√™ncia da Computa√ß√£o (PPGCC-UNESP).

O objetivo principal √© implementar uma rede neural do tipo Multi-Layer Perceptron (MLP) utilizando PyTorch, de forma manual (sem o uso de modelos prontos), para resolver problemas de classifica√ß√£o com diferentes n√≠veis de complexidade, explorando diferentes arquiteturas, fun√ß√µes de ativa√ß√£o, estrat√©gias de regulariza√ß√£o e otimizadores.

## Estrutura do Projeto

* `src/`
    * `mlp.py`: Implementa√ß√£o para o Experimento 3.1 (Dataset 2)
    * `mlp2.py`: Implementa√ß√£o para o Experimento 3.2 (Dataset 4)
* `dataset/`
    * `2/`: Dataset para o experimento 3.1 (problema n√£o-linear)
        * `test_dataset.csv`
        * `train_dataset.csv`
    * `4/`: Dataset para o experimento 3.2 (50 features, 4 classes)
        * `test_dataset.csv`
        * `train_dataset.csv`
        * `validation_dataset.csv`
* `README.md`

## Atendendo aos Requisitos da Atividade

O projeto foi desenvolvido para atender integralmente aos requisitos especificados no PDF da atividade:

### 1. Implementa√ß√£o Manual da MLP
- Uso da classe nn.Module do PyTorch
- Arquitetura flex√≠vel: permite definir o n√∫mero de camadas e neur√¥nios por camada
- Suporte √†s fun√ß√µes de ativa√ß√£o ReLU e Tanh

### 2. Treinamento e Valida√ß√£o
- Implementa√ß√£o do treinamento com SGD e Adam
- Uso de CrossEntropyLoss (multiclasse) e BCEWithLogitsLoss (bin√°rio, apenas no 3.1)
- Registro das m√©tricas: perda e acur√°cia
- Early Stopping com paci√™ncia configur√°vel
- Avalia√ß√£o no conjunto de teste

### 3. Experimentos

#### üîπ Experimento 3.1 ‚Äì Dataset 2 (src/mlp.py)
- Problema n√£o-linear com duas classes
- Arquitetura pequena e visualiza√ß√£o das fronteiras de decis√£o
- Foco em resolver a n√£o-linearidade com camadas ocultas

#### üîπ Experimento 3.2 ‚Äì Dataset 4 (src/mlp2.py)
- Execu√ß√£o automatizada de todas as combina√ß√µes de:
  - Arquiteturas (pequena, m√©dia, grande)
  - Otimizadores (SGD, Adam)
  - Ativa√ß√µes (ReLU, Tanh)
  - Regulariza√ß√£o (nenhuma, Dropout, L2)
- Resultados organizados para compara√ß√£o
- Avalia√ß√£o final no conjunto de teste

## Resultados
Os experimentos realizados mostraram que:
- Redes maiores com regulariza√ß√£o L2 tendem a apresentar o melhor desempenho.
- O otimizador Adam teve melhor estabilidade em arquiteturas mais profundas.
- A fun√ß√£o de ativa√ß√£o ReLU foi mais eficiente em redes maiores.

## Como Executar

1. Clone o reposit√≥rio:
   ``` 
   git clone https://github.com/seu-usuario/nome-do-repo.git
   cd nome-do-repo 
   ```

2. Instale as depend√™ncias:
   ``` 
   pip install torch pandas matplotlib numpy 
   ```

3. Execute os scripts conforme a atividade desejada:
   ``` 
   python src/mlp.py     # Para o Experimento 3.1
   python src/mlp2.py    # Para o Experimento 3.2 
   ```
