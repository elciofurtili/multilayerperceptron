# Atividade de Redes Neurais Artificiais â€“ Rede MLP

Este repositÃ³rio contÃ©m a implementaÃ§Ã£o completa da Atividade 3 da disciplina de Redes Neurais Artificiais do Programa de PÃ³s-GraduaÃ§Ã£o em CiÃªncia da ComputaÃ§Ã£o (PPGCC-UNESP).

O objetivo principal Ã© implementar uma rede neural do tipo Multi-Layer Perceptron (MLP) utilizando PyTorch, de forma manual (sem o uso de modelos prontos), para resolver problemas de classificaÃ§Ã£o com diferentes nÃ­veis de complexidade, explorando diferentes arquiteturas, funÃ§Ãµes de ativaÃ§Ã£o, estratÃ©gias de regularizaÃ§Ã£o e otimizadores.

---

## ğŸ”§ Estrutura do Projeto

.
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ mlp.py        # ImplementaÃ§Ã£o para o Experimento 3.1 (Dataset 2)
â”‚   â””â”€â”€ mlp2.py       # ImplementaÃ§Ã£o para o Experimento 3.2 (Dataset 4)
â”œâ”€â”€ dataset/
â”‚   â”œâ”€â”€ 2/            # Dataset para o experimento 3.1 (problema nÃ£o-linear)
â”‚   â”‚   â””â”€â”€ ...       # Arquivos do Dataset 2
â”‚   â””â”€â”€ 4/            # Dataset para o experimento 3.2 (50 features, 4 classes)
â”‚       â”œâ”€â”€ train.csv
â”‚       â”œâ”€â”€ val.csv
â”‚       â””â”€â”€ test.csv
â””â”€â”€ README.md

---

## âœ… Atendendo aos Requisitos da Atividade

O projeto foi desenvolvido para atender integralmente aos requisitos especificados no PDF da atividade:

### 1. ImplementaÃ§Ã£o Manual da MLP
- Uso da classe nn.Module do PyTorch
- Arquitetura flexÃ­vel: permite definir o nÃºmero de camadas e neurÃ´nios por camada
- Suporte Ã s funÃ§Ãµes de ativaÃ§Ã£o ReLU e Tanh

### 2. Treinamento e ValidaÃ§Ã£o
- ImplementaÃ§Ã£o do treinamento com SGD e Adam
- Uso de CrossEntropyLoss (multiclasse) e BCEWithLogitsLoss (binÃ¡rio, apenas no 3.1)
- Registro das mÃ©tricas: perda e acurÃ¡cia
- Early Stopping com paciÃªncia configurÃ¡vel
- AvaliaÃ§Ã£o no conjunto de teste

### 3. Experimentos

#### ğŸ”¹ Experimento 3.1 â€“ Dataset 2 (src/mlp.py)
- Problema nÃ£o-linear com duas classes
- Arquitetura pequena e visualizaÃ§Ã£o das fronteiras de decisÃ£o
- Foco em resolver a nÃ£o-linearidade com camadas ocultas

#### ğŸ”¹ Experimento 3.2 â€“ Dataset 4 (src/mlp2.py)
- ExecuÃ§Ã£o automatizada de todas as combinaÃ§Ãµes de:
  - Arquiteturas (pequena, mÃ©dia, grande)
  - Otimizadores (SGD, Adam)
  - AtivaÃ§Ãµes (ReLU, Tanh)
  - RegularizaÃ§Ã£o (nenhuma, Dropout, L2)
- Resultados organizados para comparaÃ§Ã£o
- AvaliaÃ§Ã£o final no conjunto de teste

---

## ğŸ“Š Resultados
Os experimentos realizados mostraram que:
- Redes maiores com regularizaÃ§Ã£o L2 tendem a apresentar o melhor desempenho.
- O otimizador Adam teve melhor estabilidade em arquiteturas mais profundas.
- A funÃ§Ã£o de ativaÃ§Ã£o ReLU foi mais eficiente em redes maiores.

---

## ğŸš€ Como Executar

1. Clone o repositÃ³rio:
   git clone https://github.com/seu-usuario/nome-do-repo.git
   cd nome-do-repo

2. Instale as dependÃªncias:
   pip install torch pandas matplotlib numpy

3. Execute os scripts conforme a atividade desejada:
   python src/mlp.py     # Para o Experimento 3.1
   python src/mlp2.py    # Para o Experimento 3.2